{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Piyushkumar1307/Text-Summarizer/blob/main/Text-Summarizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IGYMGpzx2fLE",
        "outputId": "39549a93-b1ae-48d8-fcd7-ec953d085a5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install mtranslate\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dWnQfVVa6vyM",
        "outputId": "78cbe6b5-8d62-4696-f697-881a6187517f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting mtranslate\n",
            "  Downloading mtranslate-1.8.tar.gz (2.4 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: mtranslate\n",
            "  Building wheel for mtranslate (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mtranslate: filename=mtranslate-1.8-py3-none-any.whl size=3697 sha256=08b8970a76e1d3c61cb41ddc3c4db79048d6ffa67aaef6994a65fc3c2e203a5d\n",
            "  Stored in directory: /root/.cache/pip/wheels/a2/20/13/93c62f314d4a29db25b1b24b2c38f79eb1beb558c727b894a3\n",
            "Successfully built mtranslate\n",
            "Installing collected packages: mtranslate\n",
            "Successfully installed mtranslate-1.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspellchecker"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gfOn1X097eNN",
        "outputId": "0b69dcc2-98c5-4c42-ffc0-bdebf66d9cf6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspellchecker\n",
            "  Downloading pyspellchecker-0.7.2-py3-none-any.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyspellchecker\n",
            "Successfully installed pyspellchecker-0.7.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Rouge"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3kaWmK-g6WmQ",
        "outputId": "65dd593b-eff1-4aad-8658-efea7204f3cf"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting Rouge\n",
            "  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from Rouge) (1.16.0)\n",
            "Installing collected packages: Rouge\n",
            "Successfully installed Rouge-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langdetect"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LG3eVOmFwaYU",
        "outputId": "365a977a-33cb-4693-a121-c9682da33afb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from langdetect) (1.16.0)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993243 sha256=adae6d33e13b9f8d065439ecf22de2c7620d977f537812a8875f58e730a205d8\n",
            "  Stored in directory: /root/.cache/pip/wheels/d1/c1/d9/7e068de779d863bc8f8fc9467d85e25cfe47fa5051fff1a1bb\n",
            "Successfully built langdetect\n",
            "Installing collected packages: langdetect\n",
            "Successfully installed langdetect-1.0.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import mtranslate\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from heapq import nlargest\n",
        "import time\n",
        "from spellchecker import SpellChecker\n",
        "from rouge import Rouge\n",
        "import nltk\n",
        "import langdetect\n",
        "\n",
        "# Dictionary of Indian languages and their ISO codes\n",
        "indian_languages = {\n",
        "    \"Bengali\": \"bn\",\n",
        "    \"Gujarati\": \"gu\",\n",
        "    \"Hindi\": \"hi\",\n",
        "    \"Kannada\": \"kn\",\n",
        "    \"Malayalam\": \"ml\",\n",
        "    \"Marathi\": \"mr\",\n",
        "    \"Nepali\": \"ne\",\n",
        "    \"Oriya\": \"or\",\n",
        "    \"Punjabi\": \"pa\",\n",
        "    \"Tamil\": \"ta\",\n",
        "    \"Telugu\": \"te\",\n",
        "    \"Urdu\": \"ur\"\n",
        "}\n",
        "\n",
        "def summarize(text, percentage):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = word_tokenize(text.lower())\n",
        "    freq_table = {}\n",
        "    for word in words:\n",
        "        if word not in stop_words:\n",
        "            if word in freq_table:\n",
        "                freq_table[word] += 1\n",
        "            else:\n",
        "                freq_table[word] = 1\n",
        "\n",
        "    # Perform NER and exclude recognized entities from frequency table\n",
        "    entities = nltk.ne_chunk(nltk.pos_tag(words))\n",
        "    for entity in entities:\n",
        "        if isinstance(entity, nltk.tree.Tree):\n",
        "            entity_words = [word for word, tag in entity.leaves()]\n",
        "            if len(entity_words) > 1:\n",
        "                entity_text = ' '.join(entity_words)\n",
        "                for word in entity_words:\n",
        "                    del freq_table[word]\n",
        "                freq_table[entity_text] = words.count(entity_text)\n",
        "\n",
        "    sentences = sent_tokenize(text)\n",
        "    sentence_scores = {}\n",
        "    for sent in sentences:\n",
        "        for word in word_tokenize(sent.lower()):\n",
        "            if word in freq_table:\n",
        "                if sent in sentence_scores:\n",
        "                    sentence_scores[sent] += freq_table[word]\n",
        "                else:\n",
        "                    sentence_scores[sent] = freq_table[word]\n",
        "\n",
        "    n = int(len(sentences) * percentage)\n",
        "    summary_sentences = nlargest(n, sentence_scores, key=sentence_scores.get)\n",
        "    summary = ' '.join(summary_sentences)\n",
        "    return summary\n",
        "\n",
        "\n",
        "def translate_text(text, target_language):\n",
        "    translation = mtranslate.translate(text, target_language)\n",
        "    return translation\n",
        "\n",
        "\n",
        "def main(target_language=None):\n",
        "    text = input(\"Enter text to summarize and translate: \")\n",
        "    print(\"Length of original text:\", len(text))\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = word_tokenize(text)\n",
        "    filtered_words = [word for word in words if not word in stop_words]\n",
        "    filtered_text = ' '.join(filtered_words)\n",
        "\n",
        "    # Perform spell checking and exclude recognized entities\n",
        "    spell = SpellChecker()\n",
        "    misspelled = spell.unknown(filtered_words)\n",
        "    entities = nltk.ne_chunk(nltk.pos_tag(words))\n",
        "    recognized_entities = []\n",
        "    for entity in entities:\n",
        "        if isinstance(entity, nltk.tree.Tree):\n",
        "            entity_words = [word for word, tag in entity.leaves()]\n",
        "            if len(entity_words) > 1:\n",
        "                entity_text = ' '.join(entity_words)\n",
        "                recognized_entities.append(entity_text)\n",
        "    corrections = {}\n",
        "    for word in filtered_words:\n",
        "        if word in recognized_entities:\n",
        "            corrections[word] = word\n",
        "        elif word in misspelled:\n",
        "            if word == \"'s\":\n",
        "                corrections[word] = word\n",
        "            else:\n",
        "                correction = spell.correction(word)\n",
        "                corrections[word] = correction\n",
        "        else:\n",
        "            corrections[word] = word\n",
        "    corrected_words = [corrections[word] for word in filtered_words]\n",
        "    corrected_text = ' '.join(corrected_words)\n",
        "    print(\"Corrected text:\")\n",
        "    print(corrected_text)\n",
        "    print(\"Incorrect words and their corrections:\")\n",
        "    for word in corrections:\n",
        "        if corrections[word] != word:\n",
        "            print(word, \"->\", corrections[word])\n",
        "\n",
        "    percentage = float(input(\"Enter the percentage of summarization (e.g. 0.2 for 20%): \"))\n",
        "\n",
        "    start_time = time.time()\n",
        "    summary = summarize(corrected_text, percentage)\n",
        "    end_time = time.time()\n",
        "\n",
        "    print(\"Summary:\\n\", summary)\n",
        "    print(\"Length of summary:\", len(summary))\n",
        "    print(\"Time taken:\", end_time - start_time, \"seconds\")\n",
        "\n",
        "    if target_language is None:\n",
        "    # Ask user if they want to translate the summary\n",
        "       translate_option = input(\"Do you want to translate the summary? (y/n): \")\n",
        "       if translate_option.lower() == 'y':\n",
        "          print(\"List of Indian languages:\")\n",
        "          for lang in indian_languages:\n",
        "              print(lang)\n",
        "          target_language_name = input(\"Enter target language name: \")\n",
        "          target_language = indian_languages.get(target_language_name)\n",
        "          if not target_language:\n",
        "            print(\"Invalid language name.\")\n",
        "            return\n",
        "       else:\n",
        "        print(\"Summary will not be translated.\")\n",
        "\n",
        "    else:\n",
        "        target_language_name = [k for k, v in indian_languages.items() if v == target_language][0]\n",
        "        print(\"Target language: \", target_language_name)\n",
        "\n",
        "    translated_summary = translate_text(summary, target_language)\n",
        "    print(\"Translated summary:\\n\", translated_summary)\n",
        "\n",
        "# Calculate ROUGE score\n",
        "    reference_summary = input(\"Enter the reference summary: \")\n",
        "    rouge = Rouge()\n",
        "    scores = rouge.get_scores(summary, reference_summary)\n",
        "    print(\"Accuracy: \", scores[0]['rouge-1']['f'])\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xzcnuS2q05WD",
        "outputId": "7a3f73d5-1e33-4aed-f5d2-e540cdd1c262"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter text to summarize and translate: Clint Barton's daghter vanishes and the rest of his family disintegrates due to Thanos' snapping his fingers. Nebula and Tony Stark are stranded in space, but Carol Danvers returns them to Earth. Thor beheads Thanos to avoid using the Stones for further nefarious purposes. Five years later, Scott Lang escapes from the quantum realm and discovers that his daughter Cassie is now a teenager and that Hope van Dyne has disappeared. Rogers is leading grief counseling sessions for survivors, while Romanoff is keeping watch over Earth and the rest of the univese.\n",
            "Length of original text: 561\n",
            "Corrected text:\n",
            "Clint Barton 's daughter vanishes rest family disintegrates due Thanos ' snapping fingers . Nebula Tony Stark stranded space , Carol Danvers returns Earth . Thor beheads Thanos avoid using Stones nefarious purposes . Five years later , Scott Lang escapes quantum realm discovers daughter Cassie teenager Hope van Dyne disappeared . Rogers leading grief counseling sessions survivors , Romanoff keeping watch Earth rest universe .\n",
            "Incorrect words and their corrections:\n",
            "daghter -> daughter\n",
            "univese -> universe\n",
            "Enter the percentage of summarization (e.g. 0.2 for 20%): 0.5\n",
            "Summary:\n",
            " Five years later , Scott Lang escapes quantum realm discovers daughter Cassie teenager Hope van Dyne disappeared . Rogers leading grief counseling sessions survivors , Romanoff keeping watch Earth rest universe .\n",
            "Length of summary: 212\n",
            "Time taken: 0.027144908905029297 seconds\n",
            "Do you want to translate the summary? (y/n): y\n",
            "List of Indian languages:\n",
            "Bengali\n",
            "Gujarati\n",
            "Hindi\n",
            "Kannada\n",
            "Malayalam\n",
            "Marathi\n",
            "Nepali\n",
            "Oriya\n",
            "Punjabi\n",
            "Tamil\n",
            "Telugu\n",
            "Urdu\n",
            "Enter target language name: Hindi\n",
            "Translated summary:\n",
            " पांच साल बाद, स्कॉट लैंग क्वांटम दायरे से बच निकलता है और पता चलता है कि किशोरी कैसी की बेटी होप वैन डायन गायब हो गई है। रोजर्स दु: ख परामर्श सत्र का नेतृत्व कर रहे हैं, रोमनॉफ़ अर्थ रेस्ट ब्रह्मांड पर नज़र रख रहे हैं।\n",
            "Enter the reference summary: Thanos snaps his fingers, Nebula and Tony Stark are stranded in space, Thor beheads Thanos, Scott Lang discovers Cassie is a teenager and Hope van Dyne has disappeared.\n",
            "Accuracy:  0.32142856643494905\n"
          ]
        }
      ]
    }
  ]
}